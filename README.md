# Pipeline de Extra√ß√£o, Transforma√ß√£o e Carga de dados do IMDb (Internet Movie Database)

## üìö √çndice

- [Arquitetura](#arquitetura)
- [Descri√ß√£o](#descri√ß√£o)
- [Ferramentas Utilizadas](#ferramentas-utilizadas)
- [Sobre o Projeto](#sobre-o-projeto)
  - [1. Arquivo kv.yml](#1-arquivo-kvyml)
  - [2. Arquivo pipeline.yml](#2-arquivo-pipelineyml)
    - [Tasks](#tasks)
  - [3. Arquivo subflow_basics.yml](#3-arquivo-subflow_basicsyml)
    - [Tasks](#tasks-1)
  - [4. Arquivo subflow_ratings.yml](#4-arquivo-subflow_ratingsyml)
  - [5. Arquivo subflow_episode.yml](#5-arquivo-subflow_episodeyml)
- [Alguns pontos de melhoria](#alguns-pontos-de-melhoria)

## Arquitetura

![Kestra Pipeline](imgs/pipeline.png)

## Descri√ß√£o

Este projeto implementa uma **pipeline de dados orquestrada no Kestra**, com o objetivo de **extrair**, **transformar** e **carregar** dados do [IMDb Non-Commercial Datasets](https://developer.imdb.com/non-commercial-datasets/) no **BigQuery**, utilizando o **Google Cloud como infraestrutura**. Os dados s√£o inicialmente **baixados em formato TSV**, **convertidos para Parquet**, **armazenados em um Data Lake (GCS)**, **transformados via SQL** e **carregados em tabelas gerenciadas do BigQuery**.

**Tarefas da pipeline**:

**1.** Download dos arquivos em TSV

**2.** Convers√£o dos arquivos TSV para o formato Apache Parquet

**3.** Carga dos arquivos em .parquet para o Data Lake (Bucket do Google Cloud Storage)

**4.** Tratamento dos dados via SQL (Bigquery)

**5.** Carga dos dados em tabelas do BigQuery 

## Ferramentas Utilizadas

**1. Docker** - Deploy do Orquestrador de Fluxos de Trabalho (ferramenta de pipeline)

**2. Terraform** - Provisionamento dos recursos na nuvem Google Cloud Storage (Dataset do Bigquery e Bucket do Google Cloud Storage)

**3. Kestra** - Implementa√ß√£o da pipeline de dados

**4. Pandas** - Transforma√ß√£o dos arquivos .tsv para .parquet

**5. Google Cloud Storage** - Para servir como Data Lake (Bucket)

**6. BigQuery** - Para servir como um Data Warehouse

## Sobre o Projeto

No Kestra √© poss√≠vel criar pipelines tanto atrav√©s de c√≥digo-fonte em YAML como atrav√©s de No-Code / Low-Code. Para esta pipeline **foram definidos 5 arquivos .yml, que est√£o na pasta 'flows'**.

### **1. Arquivo kv.yml**

Este arquivo cont√©m **pares chave-valor com informa√ß√µes sobre a autentica√ß√£o do acesso ao GCP**. Quando o mesmo √© executado, os valores passam a fazer parte do storage interno do Kestra (em PostgreSQL) e passam a estar dispon√≠veis para todas as pipelines do namespace. **Namespace** √© uma organiza√ß√£o l√≥gica para cada projeto e pode conter v√°rias pipelines. 

![KV](imgs/kv.png)

### **2. Arquivo pipeline.yml**

Este arquivo cont√©m o fluxo principal da pipeline. Atrav√©s da execu√ß√£o dele √© poss√≠vel **fornecer os par√¢metros da execu√ß√£o**, **fazer o download dos arquivos** para o storage do Kestra, **converter para Parquet**, **ingerir os arquivos convertidos no Data Lake** (GCS) e **decidir qual dos 3 subfluxos ser√£o executados**.

Vis√£o No-Code e Topol√≥gica do Fluxo Principal:
![Fluxo Principal](imgs/pipeline_imdb.png)

#### **Tasks**:

**download_files**

**'tsv_file'** √© uma vari√°vel que foi definida tendo como base o valor de input fornecido ao executar a pipeline. A fun√ß√£o 'render' √© necess√°ria para que o valor seja corretamente constru√≠do com base nos inputs.

```yaml
- id: download_files
    type: io.kestra.plugin.scripts.shell.Commands
    description: Baixa e salva o arquivo .tsv j√° descompactado, atrav√©s de comando de CLI.
    outputFiles:
      - "*.tsv"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    commands:
      - wget -qO- https://datasets.imdbws.com/{{render(vars.tsv_file)}}.gz | gunzip > {{render(vars.tsv_file)}}
```

**tsv_to_parquet**

CSV's, e por consequ√™ncia TSV's, n√£o possuem metadados inerentes, portanto trata-se apenas de arquivos de texto. Ler com o pandas antes e depois converter para o parquet garante que a infer√™ncia de tipos do pandas seja utilizada e que os tipos de dados fa√ßam parte dos metadados do parquet.  

```yaml
- id: tsv_to_parquet
    type: io.kestra.plugin.scripts.python.Script
    description: Converte os arquivos .tsv em parquet para economizar espa√ßo no data lake.
    beforeCommands:
      - pip install pandas fastparquet
    outputFiles:
      - "*.parquet"
    script: |
      import pandas as pd

      # Obtendo o caminho do arquivo baixado
      tsv_data = "{{render(vars.tsv_data)}}"

      # Carregando o arquivo TSV
      df = pd.read_csv(tsv_data, sep="\t", low_memory=False)

      # Salvando como Parquet
      df.to_parquet('{{render(vars.parquet_file)}}', index = False, engine = "fastparquet")
```

**ingest_parquet_on_data_lake**

Com as credenciais configuradas, basta apontar para o endere√ßo do bucket.

```yaml
- id: ingest_parquet_on_data_lake
    type: io.kestra.plugin.gcp.gcs.Upload
    description: Carrega os arquivos parquet no data lake (bucket do GCS).
    from: "{{render(vars.parquet_data)}}"
    to: "{{render(vars.gcs_file_parquet)}}"
```

**if_ratings**

Condi√ß√£o para caso o input seja 'ratings', que √© o nome de um arquivo baixado. Um subflow √© executado juntamente com os par√¢metros necess√°rios.

```yaml
- id: if_ratings
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.file_name == 'ratings'}}"
    description: |
        Executa um subflow dedicado ao arquivo 'ratings', que cont√©m as notas dadas pelos usu√°rios aos filmes/s√©ries.
        Essa tabela n√£o precisa ser particionada / clusterizada, pois seu tamanho √© pouco mais de 30 MB.
    then:
      - id: subflow_ratings_pipeline
        type: io.kestra.plugin.core.flow.Subflow
        flowId: ratings_pipeline
        namespace: zoomcamp
        inputs:
          table: "{{render(vars.table)}}"
          gcs_file_parquet: "{{render(vars.gcs_file_parquet)}}"
          parquet_file: "{{render(vars.parquet_file)}}"
        wait: true # a pipeline principal ir√° esperar a execu√ß√£o do subflow finalizar para prosseguir
        transmitFailed: true # o pipeline principal ir√° falhar caso a execu√ß√£o do subflow falhe
```

**if_basics**

Id√™ntico √† task if_ratings, com algumas adapta√ß√µes.

**if_episode**

Id√™ntico √† task if_ratings, com algumas adapta√ß√µes.

**delete_generated_files**

Depois que os dados s√£o carregados no data lake, n√£o se faz necess√°rio mant√™-los localmente.

```yaml
  - id: delete_generated_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: Exclui os arquivos produzidos pela pipeline.
    disabled: false
```

### **3. Arquivo subflow_basics.yml**

**Os 3 subflows seguem praticamente a mesma estrutura**, portanto o aprofundamento ser√° feito apenas neste.

Vis√£o no e low code da pipeline.
![Fluxo arquivo basics](imgs/basics.png)

#### **Tasks**:

**bq_basics_table**

Cria√ß√£o da tabela definitiva. **A clusteriza√ß√£o foi feita para otimizar consultas que fa√ßam agrupamentos utilizando a coluna titleType**.

As colunas **filename** e **unique_row_id** n√£o s√£o originais do dataset, a primeira foi criada apenas para guardar o nome do arquivo de origem, a segunda foi para servir de identificador √∫nico de um registro.

```yaml
    - id: bq_basics_table
        type: io.kestra.plugin.gcp.bigquery.Query
        description: Cria a tabela principal no BigQuery.
        sql: |
          CREATE TABLE IF NOT EXISTS `{{kv('gcp_project_id')}}.{{inputs.table}}`
          (
              unique_row_id BYTES,
              filename STRING,
              tconst STRING,
              titleType STRING,
              primaryTitle STRING,
              originalTitle STRING,
              isAdult BYTES,      
              startYear BYTES,
              endYear BYTES,
              runtimeMinutes BYTES,
              genres STRING
          )
          CLUSTER BY 
             titleType;
```

**bq_basics_table_external**

Criando uma tabela com base no arquivo parquet presente no data lake.

```yaml
    - id: bq_basics_table_external
        type: io.kestra.plugin.gcp.bigquery.Query
        description: Cria a tabela externa com os dados do parquet do data lake.
        sql: |
          CREATE OR REPLACE EXTERNAL TABLE `{{kv('gcp_project_id')}}.{{inputs.table}}_external`
          (
              unique_row_id BYTES,
              filename STRING,
              tconst STRING,
              titleType STRING,
              primaryTitle STRING,
              originalTitle STRING,
              isAdult BYTES,      
              startYear BYTES,
              endYear BYTES,
              runtimeMinutes BYTES,
              genres STRING
          )
          OPTIONS (
              format = 'PARQUET',
              uris = ['{{inputs.gcs_file_parquet}}']
          );
```

**bq_basics_tmp**

Essa **tabela tempor√°ria funciona como uma tabela de staging**. **Aqui √© calculado o identificador √∫nico do registro com base em v√°rias colunas**. O valor do **hash** √© conseguido com a fun√ß√£o MD5.

```yaml
- id: bq_basics_table_tmp
        type: io.kestra.plugin.gcp.bigquery.Query
        description: Cria uma tabela tempor√°ria para ser mergeada com a tabela principal. Aqui eu crio um hash para identificar um registro de forma √∫nica.
        sql: |
          CREATE OR REPLACE TABLE `{{kv('gcp_project_id')}}.{{inputs.table}}_tmp`
          AS
          SELECT
            MD5(CONCAT(
              COALESCE(CAST(tconst AS STRING), ""),
              COALESCE(CAST(primaryTitle AS STRING), ""),
              COALESCE(CAST(originalTitle AS STRING), ""),
              COALESCE(CAST(startYear AS STRING), "")
            )) AS unique_row_id,
            "{{inputs.parquet_file}}" AS filename,
              tconst,
              titleType,
              primaryTitle,
              originalTitle,
              isAdult,      
              startYear,
              endYear,
              runtimeMinutes,
              genres
          FROM `{{kv('gcp_project_id')}}.{{inputs.table}}_external`;
```

**bq_basics_merge**

Aqui √© feito um **merge entre a tabela definitiva e a tabela tempor√°ria utilizando o hash de ambas como chave**. Fun√ß√µes hash sempre devolvem a mesma chave se os mesmos inputs forem fornecidos, portanto se um registro da tabela tempor√°ria j√° estiver na tabela final, ele ser√° desconsiderado.

```yaml
      - id: bq_basics_table_merge
        type: io.kestra.plugin.gcp.bigquery.Query
        description: Merge entre a tabela principal e a tabela tempor√°ria.
        sql: |
          MERGE INTO 
            `{{kv('gcp_project_id')}}.{{inputs.table}}` T
          USING 
            `{{kv('gcp_project_id')}}.{{inputs.table}}_tmp` S
          ON 
            T.unique_row_id = S.unique_row_id
          WHEN NOT MATCHED THEN
          INSERT 
            ( unique_row_id, 
              filename, 
              tconst,
              titleType,
              primaryTitle,
              originalTitle,
              isAdult,      
              startYear,
              endYear,
              runtimeMinutes,
              genres )
          VALUES 
            ( S.unique_row_id, 
              S.filename, 
              S.tconst,
              S.titleType,
              S.primaryTitle,
              S.originalTitle,
              S.isAdult,      
              S.startYear,
              S.endYear,
              S.runtimeMinutes,
              S.genres );
```

**bq_basics_table_tmp_truncate**

Por fim, a tabela tempor√°ria e a tabela externa s√£o exclu√≠das.

```yaml
      - id: bq_basics_table_tmp_truncate
        type: io.kestra.plugin.gcp.bigquery.Query
        description: Excluindo a tabela tempor√°ria e a tabela externa.
        sql: |
          DROP TABLE `{{kv('gcp_project_id')}}.{{inputs.table}}_tmp`;
          DROP TABLE `{{kv('gcp_project_id')}}.{{inputs.table}}_external`;
```

### **4. Arquivo subflow_ratings.yml**

Id√™ntico ao arquivo subflow_basics, com algumas adapta√ß√µes.

Vis√£o no e low code da pipeline.
![Fluxo arquivo ratings](imgs/ratings.png)

### **5. Arquivo subflow_episode.yml**

Id√™ntico ao arquivo subflow_basics, com algumas adapta√ß√µes.

Vis√£o no e low code da pipeline.
![Fluxo arquivo episodes](imgs/episodes.png)

## Alguns pontos de melhoria:

**1. Modelagem Dimensional** 

Organizar as tabelas finais no formato estrela ou snowflake, facilitando an√°lises OLAP.

**2. Tratamento de Dados** 

Implementar regras para lidar com campos nulos ou valores ausentes, melhorando a qualidade dos dados.

**3. Redu√ß√£o de redund√¢ncia** 

Modularizar trechos repetidos entre os subflows para facilitar manuten√ß√£o e reutiliza√ß√£o de c√≥digo.

**4. Visualiza√ß√£o de Dados** 

Desenvolver dashboard para acompanhar m√©tricas como n√∫mero de filmes por ano, g√™neros mais comuns, notas m√©dias etc.